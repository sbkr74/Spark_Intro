```py
from pyspark.sql import SparkSession
from pyspark import SparkContext as sc

spark = SparkSession.builder.appName("text conversion").getOrCreate()

# Relative File path for CSV file.
file_Path = r"Docs\files\customers-100.csv"

cust_df = spark.read.option("header","True").option("inferSchema","True").csv(file_Path)

# top record
first_rec = cust_df.first()
rdd = sc.parallelize(first_rec)
df_first = rdd.toDF()
df_first.show()
```
Above code has an issue in the line:

```python
rdd = sc.parallelize(first_rec)
```

### Issues:
1. **Incorrect SparkContext Usage:**  
   - You're trying to use `SparkContext` as `sc`, but in your code, you haven't created a `SparkContext` instance. Instead, you're using `SparkSession` (`spark`), which already includes a `SparkContext` (`spark.sparkContext`).
   - Also, `SparkContext` is not needed when working with `DataFrame` operations.

2. **Invalid `parallelize()` Input:**  
   - `first_rec` is a **single row** (which is a `Row` object), not an iterable. `sc.parallelize(first_rec)` expects an **iterable** (e.g., list), but `first_rec` is not iterable.

3. **Incorrect Conversion from RDD to DataFrame:**  
   - `rdd.toDF()` will not work as expected because `first_rec` is a single row, not a list of rows.

---

### Corrected Code:
```python
from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.appName("text conversion").getOrCreate()

# Relative File path for CSV file.
file_Path = r"Docs/files/customers-100.csv"  # Use forward slashes for better cross-platform support

# Read CSV into DataFrame
cust_df = spark.read.option("header", "True").option("inferSchema", "True").csv(file_Path)

# Get first record as a Row object
first_rec = [cust_df.first()]  # Convert Row to a list

# Convert to DataFrame
df_first = spark.createDataFrame(first_rec)

# Show result
df_first.show()
```

### Key Fixes:
- **Used `spark.createDataFrame([first_rec])`** to create a DataFrame from a single row.
- **Replaced `Docs\files\customers-100.csv` with `Docs/files/customers-100.csv`** to ensure cross-platform compatibility.
- **Used `spark.sparkContext.parallelize([...])` instead of `sc.parallelize(...)`** (but it's unnecessary here).

---