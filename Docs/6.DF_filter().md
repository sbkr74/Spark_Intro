### **Issue in Code**
```python
filter_df = df.filter("Age == 50" or "Age == 30")
```
- The expression inside `.filter()` should be a valid SQL or PySpark expression.
- `"Age == 50"` is a string, which is always `True` in Python when used in `or` conditions.
- `"Age == 50" or "Age == 30"` is evaluated by Python **before** being passed to `.filter()`, resulting in only `"Age == 50"` being considered.

### **Correct Approach**

#### **Using SQL Expression as a String**
You should use a proper SQL condition within `.filter()`:
```python
filter_df = df.filter("Age = 50 OR Age = 30")
filter_df.show()
```

#### **Using PySpark Column Expressions**
A more recommended approach is using the `col()` function from `pyspark.sql.functions`:
```python
from pyspark.sql.functions import col

filter_df = df.filter((col("Age") == 50) | (col("Age") == 30))
filter_df.show()
```
- This method is **preferred** because it avoids SQL string parsing errors and improves readability.


---

### **Issue in  Code**
```python
filter_df = df.filter("Age == 50 and Name == 'NULL'")
```
- **Incorrect usage of `==` in SQL expressions**:  
  - In SQL-style expressions for PySpark, **single equals (`=`) should be used instead of double equals (`==`)**.
- **Possible misunderstanding of 'NULL'**:
  - If you are checking for missing (null) values in the `Name` column, using `"Name == 'NULL'"` won't work because `NULL` is a special SQL value.
  - `'NULL'` (as a string) is different from `NULL` (actual missing value).

---

### **Corrected Code**
#### **1️⃣ If You Want to Match the String `'NULL'` (as a literal text)**  
If `Name` contains the literal string `'NULL'`, use:
```python
filter_df = df.filter("Age = 50 AND Name = 'NULL'")
filter_df.show()
```
✅ This correctly applies an SQL-style filter.

#### **2️⃣ If You Want to Check for `NULL` Values (Missing Data)**
If you want to filter rows where `Name` is actually `NULL` (missing value), use **`IS NULL`** instead:
```python
filter_df = df.filter("Age = 50 AND Name IS NULL")
filter_df.show()
```
✅ This filters rows where `Age` is 50 and `Name` is truly `NULL` (not a string `'NULL'`).

#### **3️⃣ Using PySpark Column Expressions (Recommended Approach)**
If you prefer using PySpark functions (which are more robust and readable), use:
```python
from pyspark.sql.functions import col

filter_df = df.filter((col("Age") == 50) & (col("Name").isNull()))
filter_df.show()
```
✅ This is the best practice as it avoids SQL string parsing issues.

---
# Filter top 10 records
Here are **three alternative ways** to store only the top 10 records in `df` in PySpark:  

### **1️⃣ Using `.take(10)` and `spark.createDataFrame()`**
```python
df = spark.createDataFrame(df.take(10), df.schema)
df.show()
```
✅ This approach fetches the top 10 rows as a list (`take(10)`) and then reconstructs the DataFrame.

---

### **2️⃣ Using `.orderBy()` with `.limit(10)` (if ordering is needed)**
If you want the top 10 based on a specific column (e.g., Age in descending order):
```python
from pyspark.sql.functions import col

df = df.orderBy(col("Age").desc()).limit(10)
df.show()
```
✅ This sorts `df` by `Age` in descending order before limiting to 10 rows.

---

### **3️⃣ Using `.head(10)` and `spark.createDataFrame()`**
```python
df = spark.createDataFrame(df.head(10), df.schema)
df.show()
```
✅ Similar to `.take(10)`, but `.head(10)` can sometimes be more efficient.

---

If your DataFrame has **fewer than 10 records**, the methods I mentioned earlier will still work **without errors**—they will simply return all available rows.  

### **Behavior of Each Method with <10 Records:**
| **Method**                 | **Behavior When Records < 10** |
|----------------------------|--------------------------------|
| `df.limit(10)`             | Returns all available rows |
| `df.take(10)` + `createDataFrame()` | Returns all available rows |
| `df.head(10)` + `createDataFrame()` | Returns all available rows |
| `df.orderBy().limit(10)`   | Returns all available rows |

---

The **`.desc()`** method needs to be applied to the column inside `orderBy()`, not outside.  

### **❌ Incorrect Code:**
```python
df_bottom = df.orderBy('Record ID').desc()  # ❌ Wrong Syntax
```
This will raise an error because `.desc()` is not a method of `DataFrame`.

---

### **✅ Corrected Code:**
#### **1️⃣ Using `col()` with `.desc()` (Recommended)**
```python
from pyspark.sql.functions import col

df_bottom = df.orderBy(col("Record ID").desc())
df_bottom.show()
```
✔ This correctly sorts `df` in descending order by `"Record ID"`.

#### **2️⃣ Using SQL-style String (Alternative)**
```python
df_bottom = df.orderBy("Record ID DESC")
df_bottom.show()
```
✔ This also works but is **less flexible** than the first method.

---

